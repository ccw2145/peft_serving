{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767fd7de-26e2-4668-b216-27da1954bfee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Serving Fine Tuned Gemma Model with multiple LoRA adapters on Databricks \n",
    "\n",
    "This is a tutorial to show how to serve [`gemma-2-2b-it`](https://huggingface.co/google/gemma-2-2b-it) with multiple LoRA adpaters on Databricks Model Serving.\n",
    "\n",
    "Environment for this notebook:\n",
    "- Runtime: 16.1 GPU ML Runtime\n",
    "- Instance: Tested on `g5.8xlarge` for AWS, smaller GPU cluster should also work\n",
    "- MLFlow 2.15\n",
    "\n",
    "Serving Endpoint requirement:\n",
    "- For this example, 1 T4 GPU is sufficient without further quantization (GPU Small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c5e29d-8cc6-402e-9593-0b893acc00fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install required packages\n",
    "\n",
    "Run the cells below to setup and install the required libraries. Since gemma-2-2b-it is small enought to fit in the cluster, we are not loading a quantized base model. However, for larger models (e.g. Llama 7 or 8B models), we can use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also need `accelerate`, `peft`, `transformers` to lload the base model and PEFT adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca4ea5a-33ff-4475-8b3b-bc4262dc2b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.45\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.12/site-packages (from bitsandbytes==0.45) (2.5.0+cu124)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from bitsandbytes==0.45) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /databricks/python3/lib/python3.12/site-packages (from bitsandbytes==0.45) (4.11.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (3.13.1)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (3.1.4)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (2023.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (3.1.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.45) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch->bitsandbytes==0.45) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch->bitsandbytes==0.45) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes==0.45) (2.1.3)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/69.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/69.1 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.7/69.1 MB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\n\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/69.1 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/69.1 MB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.8/69.1 MB\u001B[0m \u001B[31m16.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.9/69.1 MB\u001B[0m \u001B[31m18.5 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.1/69.1 MB\u001B[0m \u001B[31m21.2 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\n\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.8/69.1 MB\u001B[0m \u001B[31m24.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\n\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.8/69.1 MB\u001B[0m \u001B[31m28.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.4/69.1 MB\u001B[0m \u001B[31m41.0 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.7/69.1 MB\u001B[0m \u001B[31m64.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.8/69.1 MB\u001B[0m \u001B[31m93.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.0/69.1 MB\u001B[0m \u001B[31m128.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.2/69.1 MB\u001B[0m \u001B[31m152.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m34.6/69.1 MB\u001B[0m \u001B[31m156.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.3/69.1 MB\u001B[0m \u001B[31m164.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/69.1 MB\u001B[0m \u001B[31m167.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m51.7/69.1 MB\u001B[0m \u001B[31m167.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m57.4/69.1 MB\u001B[0m \u001B[31m167.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m63.0/69.1 MB\u001B[0m \u001B[31m165.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m68.8/69.1 MB\u001B[0m \u001B[31m168.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m69.1/69.1 MB\u001B[0m \u001B[31m164.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m69.1/69.1 MB\u001B[0m \u001B[31m164.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m69.1/69.1 MB\u001B[0m \u001B[31m164.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m69.1/69.1 MB\u001B[0m \u001B[31m164.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m69.1/69.1 MB\u001B[0m \u001B[31m44.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting peft\n  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from peft) (24.1)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.12/site-packages (from peft) (5.9.0)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.12/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /databricks/python3/lib/python3.12/site-packages (from peft) (2.5.0+cu124)\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.12/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /databricks/python3/lib/python3.12/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /databricks/python3/lib/python3.12/site-packages (from peft) (0.4.4)\nCollecting huggingface_hub>=0.25.0 (from peft)\n  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2023.5.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.11.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers->peft) (2023.10.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /databricks/python3/lib/python3.12/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.6.2)\nDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/411.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m409.6/411.1 kB\u001B[0m \u001B[31m17.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m411.1/411.1 kB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/481.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m481.4/481.4 kB\u001B[0m \u001B[31m29.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: huggingface_hub, peft\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.24.5\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cd4e19dd-a617-4e48-94d1-f2f0e478ebef\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\nSuccessfully installed huggingface_hub-0.30.2 peft-0.15.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.12/site-packages (4.46.3)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cd4e19dd-a617-4e48-94d1-f2f0e478ebef/lib/python3.12/site-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.32.2)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.12/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/10.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/10.4 MB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.8/10.4 MB\u001B[0m \u001B[31m55.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m9.2/10.4 MB\u001B[0m \u001B[31m88.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m10.4/10.4 MB\u001B[0m \u001B[31m109.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.4/10.4 MB\u001B[0m \u001B[31m85.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m96.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cd4e19dd-a617-4e48-94d1-f2f0e478ebef\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Not uninstalling transformers at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cd4e19dd-a617-4e48-94d1-f2f0e478ebef\n    Can't uninstall 'transformers'. No files were found to uninstall.\nSuccessfully installed tokenizers-0.21.1 transformers-4.51.3\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes==0.45\n",
    "%pip install accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U transformers\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6100bd6-a927-4f55-bd55-07617595cbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] ='max_split_size_mb:128'\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed01c028-7468-4c18-9477-75a609431ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6171291-bce0-488a-9bd2-ccde21e0d619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading the model\n",
    "\n",
    "In this section we will load the [gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) model and a few popular open source adapters from Huggingface and save to Unity Catalog Volumes.\n",
    "\n",
    "Adapters we will be using here:\n",
    "  - [google-cloud-partnership/gemma-2-2b-it-lora-sql](https://huggingface.co/google-cloud-partnership/gemma-2-2b-it-lora-sql)\n",
    "  - [google-cloud-partnership/gemma-2-2b-it-lora-jap-en](https://huggingface.co/google-cloud-partnership/gemma-2-2b-it-lora-jap-en)\n",
    "  - [google-cloud-partnership/gemma-2-2b-it-lora-magicoder](https://huggingface.co/google-cloud-partnership/gemma-2-2b-it-lora-magicoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b04249-cd47-4d94-a92f-a63596f2ddcd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Catalog to use"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog = \"cindy_demo_catalog\"\n",
    "schema = \"llm_fine_tuning\"\n",
    "volume = \"hf_models\"\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14cd1ed-83e6-4b6f-af0e-a19fc8e8b9a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Specify model and adapters paths"
    }
   },
   "outputs": [],
   "source": [
    "base_model_path = f'/Volumes/{catalog}/{schema}/{volume}/gemma-2-2b-it'\n",
    "adapters_path = f'/Volumes/{catalog}/{schema}/{volume}/adapters' # Directory to store all adapters\n",
    "adapters_mapping_path = f'/Volumes/{catalog}/{schema}/{volume}/adapters_mapping.json' # Mapping of adapters to model names\n",
    "adapters_mapping = {'sql' :'gemma-2-2b-it-lora-sql',\n",
    "                    'japanese': 'gemma-2-2b-it-lora-jap-en',\n",
    "                    'coder': 'gemma-2-2b-it-lora-coder'\n",
    "                    }\n",
    "\n",
    "# base_tokenizer_path = f'/Volumes/{catalog}/{schema}/{volume}/gemma-2-2b-it-tokenzier' ## Specify this if tokenizer is not stored with the base model and has a different path\n",
    "\n",
    "import os\n",
    "if not os.path.exists(adapters_path):\n",
    "    dbutils.fs.mkdirs(adapters_path)\n",
    "    \n",
    "import json\n",
    "with open(adapters_mapping_path, 'w') as f:\n",
    "    json.dump(adapters_mapping, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b1e3fd-d9bc-4913-a817-373f01534901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Download base model and adapters to Unity Catalog Volumes\n",
    "- Requires Huggingface Token with access to use gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f373e17-9c12-4879-8c10-80275c9da766",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add HF token if needed"
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\"huggingface_token\", \"\", \"Enter Parameter\")\n",
    "# os.environ['HF_TOKEN'] = dbutils.widgets.get(\"huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05696cce-33f7-4714-8634-9172c3d79075",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download models to UC Volumes"
    }
   },
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# # Download base Gemma model\n",
    "# base_model_path = snapshot_download(repo_id=\"google/gemma-2-2b-it\", local_dir=base_model_path)\n",
    "\n",
    "# # Download LoRA adapters\n",
    "# lora_sql_path = snapshot_download(repo_id=\"google-cloud-partnership/gemma-2-2b-it-lora-sql\", local_dir=f\"{adapters_path}/gemma-2-2b-it-lora-sql\")\n",
    "# lora_jap_en_path = snapshot_download(repo_id=\"google-cloud-partnership/gemma-2-2b-it-lora-jap-en\", local_dir=f\"{adapters_path}/gemma-2-2b-it-lora-jap-en\")\n",
    "# lora_coder_path = snapshot_download(repo_id='google-cloud-partnership/gemma-2-2b-it-lora-magicoder', local_dir=f\"{adapters_path}/gemma-2-2b-it-lora-coder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "040401ab-b246-4b1a-a430-6072ea50be0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Load all adapters and test peft model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32ee337-e942-41b7-bddf-ec18540ec62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 21:53:15.978486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745617995.992706    7221 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745617995.997525    7221 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-25 21:53:16.015075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 21:53:22,562] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n/usr/bin/ld: cannot find -lcufile: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f36dd89aa814a168bce6406757e1b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Peft Model sql\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-a5658a4a-3ae2-4426-8151-8f726003866c/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Peft Model japanese\nloaded Peft Model coder\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "# from peft import PeftModel\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map=\"cuda:0\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "# peft_model = base_model \n",
    "\n",
    "# for name, path in adapters_mapping.items():\n",
    "#   # This loads the adapter onto the model under the provided adapter name.\n",
    "#   peft_model.load_adapter(f\"{adapters_path}/{path}\", adapter_name=name)\n",
    "#   print('loaded Peft Model',name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17921749-587e-46eb-b610-425227ac9958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"what is databricks?\\n\\n```sql\\nCREATE TABLE companies (id INT, name TEXT, industry TEXT, founding_year INT, founder_gender TEXT);\\nINSERT INTO companies (id, name, industry, founding_year, founder_gender) VALUES (1, 'Acme Inc', 'Tech', 2010, 'Female');\\nINSERT INTO companies (id, name, industry, founding_year, founder_gender) VALUES (2, 'Beta Corp', 'Tech', 2\",\n",
       " 'what is ML model performance for each model, and which models have the highest and lowest performance?\\nmodel\\n```sql\\nSELECT model_name, performance_score FROM model_performance ORDER BY performance_score DESC LIMIT 1;\\nSELECT model_name, performance_score FROM model_performance ORDER BY performance_score LIMIT 1;```\\nThis query calculates the performance score for each model and returns the model with the highest and lowest performance.\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84366ff8-bcea-4aed-8a50-9460f5000ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## peft_model.delete_adapter(\"sql\") ## To remove an adapter from peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd0a82b-9921-4383-bd01-fc62d1855dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create MLFlow PyFunc Model with Multiple Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea0b356-a293-46d2-b25f-0609a655873e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 19:14:06.049506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745694846.064933    2691 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745694846.069805    2691 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-26 19:14:06.088400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 19:14:12,777] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n/usr/bin/ld: cannot find -lcufile: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mlflow\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "class FINETUNED_QLORA(mlflow.pyfunc.PythonModel):\n",
    "    # Load base model, tokenizer, and adapters.\n",
    "    def load_context(self, context):\n",
    "        import json\n",
    "        import os\n",
    "\n",
    "        ## Uncomment this to load a quanitized model, requires less memory, slower inference due to de-quant overhead\n",
    "        # bnb_config = BitsAndBytesConfig(\n",
    "        # load_in_4bit=True,\n",
    "        # bnb_4bit_quant_type=\"nf4\",\n",
    "        # # bnb_4bit_use_double_quant=True,\n",
    "        # bnb_4bit_compute_dtype=torch.float16,\n",
    "        # )\n",
    "\n",
    "        # Load the tokenizer and set the pad token to the EOS token.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(context.artifacts['base_model'])\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load the base model (using 4-bit quantization in this example).\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            context.artifacts['base_model'], \n",
    "            return_dict=True, \n",
    "            # quantization_config=bnb_config, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\": 0})\n",
    "        \n",
    "        # Load PEFT adapters from a dictionary artifact.\n",
    "        with open(context.artifacts[\"adapters_mapping\"], \"r\") as f:\n",
    "            self.adapters_mapping = json.load(f)\n",
    "\n",
    "        print('loaded adapter mappings')\n",
    "       \n",
    "        self.model = self.base_model\n",
    "  \n",
    "        for adapter_name, adapter_path in self.adapters_mapping.items():\n",
    "            self.model.load_adapter(f\"{context.artifacts[\"adapters\"]}/{adapter_path}\", adapter_name=adapter_name)\n",
    "            print('loaded Peft Model',f\"{context.artifacts[\"adapters\"]}/{adapter_path}\")\n",
    "\n",
    "        ## Set the model to evaluation mode. Use this for a merged model\n",
    "        # self.model.eval()\n",
    "\n",
    "        self.model.config.use_cache = False\n",
    "\n",
    "    def predict(self, context, model_input, params):\n",
    "        # Handle single or batch prompts, input should be a list[str]\n",
    "        prompts = model_input.get(\"prompts\")[0]\n",
    "\n",
    "        print('input:', prompts)\n",
    "        \n",
    "        temperature = float(params.get('temperature', 0.1))\n",
    "        max_tokens = int(params.get('max_tokens', 100))\n",
    "        adapter_name = params.get('adapter', 'sql')\n",
    "        print( 'params: ', temperature, max_tokens, adapter_name)\n",
    "\n",
    "        # Activate the desired adapter if provided.\n",
    "        if adapter_name in list(self.adapters_mapping.keys()):\n",
    "          self.model.set_adapter(adapter_name)\n",
    "\n",
    "        else:\n",
    "          print('no adapter found')\n",
    "          generated_text = 'no adapter found'\n",
    "          return generated_text\n",
    "        \n",
    "        # Tokenize the input prompt with padding and truncation, and move to CUDA.\n",
    "        batch = self.tokenizer(text=prompts, padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output_tokens = self.model.generate(\n",
    "                input_ids=batch.input_ids, \n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text.\n",
    "        generated_texts = self.tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "        return generated_texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "639cc6af-30f4-452f-bc75-df9304dbb1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Test MLFlow PyFunc Model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70e5da5-88d1-47d0-b934-3e7243998f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a local model context object with required artifact paths for testing\n",
    "artifacts = {\n",
    "    \"base_model\": base_model_path,  \n",
    "    \"adapters_mapping\": adapters_mapping_path,\n",
    "    \"adapters\": adapters_path\n",
    "            }\n",
    "\n",
    "class ModelContext:\n",
    "    def __init__(self):\n",
    "        self.artifacts = artifacts\n",
    "# Instantiate a dummy context.\n",
    "dummy_context = ModelContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec29c321-0d41-4bc0-a5d7-c1318909ba85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b9d353dd994842b669d227804cf16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded adapter mappings\nloaded Peft Model /Volumes/cindy_demo_catalog/llm_fine_tuning/hf_models/adapters/gemma-2-2b-it-lora-sql\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-cd4e19dd-a617-4e48-94d1-f2f0e478ebef/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Peft Model /Volumes/cindy_demo_catalog/llm_fine_tuning/hf_models/adapters/gemma-2-2b-it-lora-jap-en\nloaded Peft Model /Volumes/cindy_demo_catalog/llm_fine_tuning/hf_models/adapters/gemma-2-2b-it-lora-coder\n"
     ]
    }
   ],
   "source": [
    "# Instantiate your pyfunc wrapper and load the context.\n",
    "finetuned_model = FINETUNED_QLORA()\n",
    "finetuned_model.load_context(dummy_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d64f5e-e1b3-42f1-a8e0-e49e4df94a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prediction...\ninput: what is Databricks?\nparams:  0.1 100 sql\nGenerated Output: [\"what is Databricks?\\n\\n```sql\\nSELECT * FROM Databricks;```\\nThis query retrieves all records from the 'Databricks' table.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "test_input =  {'prompts': [\"what is Databricks?\", \"what's ML\"]}\n",
    "params = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"sql\"\n",
    "}\n",
    "# Run a prediction and display the output.\n",
    "print(\"Testing prediction...\")\n",
    "generated_output = finetuned_model.predict(dummy_context, test_input, params)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be35c21-558d-4454-aa01-af069bd7db52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"what is Databricks?\\n\\n```sql\\nSELECT * FROM Databricks;```\\nThis query retrieves all records from the 'Databricks' table.\\n\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488b8c0b-845c-47fa-a7e0-35a6c0231c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prediction...\ninput: データブリックスとは\nparams:  0.1 100 sql\nGenerated Output: ['データブリックスとは、データの統合と分析を支援するオープンソースのツールです。\\n\\n**データブリックスの主な機能**\\n\\n* データの統合: データブリックスは、さまざまなデータソースからデータを収集し、統合する機能を提供します。\\n* データの分析: データブリックスは、統合されたデータを分析する機能を提供します。\\n* データの可視化: データブリックスは、分析結果を可視化する機能を提供します。\\n\\n**データブリックスの利点']\n"
     ]
    }
   ],
   "source": [
    "test_input = {\n",
    "    \"prompts\": [\"データブリックスとは\"],\n",
    "}\n",
    "\n",
    "# Run a prediction and display the output.\n",
    "print(\"Testing prediction...\")\n",
    "generated_output = finetuned_model.predict(dummy_context, test_input, params)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0a7f5d-7e26-45b4-8853-6daef6b490a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prediction...\ninput: ['generate some pandas code to create a dataframe'] params:  0.1 300 coder\nGenerated Output: [\"generate some pandas code to create a dataframe with the following columns:\\n    - 'date': a date column with values from 1990 to 2010, incremented by 10 years each time\\n    - 'value': a column with random values between 0 and 100\\n    - 'category': a column with values from 'A' to 'F'\\n\\nYour task is to write the pandas code to create the dataframe with the specified columns and data.\\nmodel\\n```python\\nimport pandas as pd\\n\\n# Create a dataframe with the specified columns\\ndata = {\\n    'date': pd.date_range(start='1990-01-01', periods=101, freq='10Y'),\\n    'value': np.random.rand(101),\\n    'category': np.random.choice(['A', 'B', 'C', 'D', 'E', 'F'], size=101)\\n}\\ndf = pd.DataFrame(data)\\n\\n# Print the created dataframe\\nprint(df)\\n```\\n\\nThis solution uses the pandas library to create a dataframe with the specified columns and data. The `pd.date_range` function is used to generate the 'date' column with values from 1990 to 2010, incremented by 10 years each time. The 'value' column contains random values between \"]\n"
     ]
    }
   ],
   "source": [
    "test_input = {\n",
    "    \"prompts\": [\"generate some pandas code to create a dataframe\"]\n",
    "}\n",
    "\n",
    "# Run a prediction and display the output.\n",
    "print(\"Testing prediction...\")\n",
    "generated_output = finetuned_model.predict(dummy_context, test_input, params)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "906ee0e4-2bd4-4fb2-b131-ce43058a8ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log to MLFlow + Register model in UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa3cfce-5170-4f03-9461-9fa93bbd3954",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Input Example and Model Signature"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['prompts': Array(string) (required)]\n",
       "outputs: \n",
       "  ['generated_texts': Array(string) (required)]\n",
       "params: \n",
       "  ['temperature': double (default: 0.1), 'max_tokens': long (default: 100), 'adapter': string (default: sql)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "from mlflow.types.schema import Array, DataType, Schema\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set mlflow registry to databricks-uc\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# Specify an input example that conforms to the input schema for the task.\n",
    "import numpy as np\n",
    "input_data={\"prompts\": [\"what is Databricks?\", \"what's ML\"]}\n",
    "               \n",
    "params = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"sql\"\n",
    "}\n",
    "input_example = (input_data, params)\n",
    "\n",
    "output_example = {\"generated_texts\": [\"what is Databricks?\\n\\n```sql\\nSELECT * FROM Databricks;```\\nThis query retrieves all records from the 'Databricks' table.\\n\", \"what's ML model performance for each model?\\nmodel\\n```sql\\nSELECT model_name, performance_score FROM model_performance;```\\nThis query retrieves the ML model performance for each model by selecting the model_name and performance_score columns from the model_performance table.\\n\"]}\n",
    "\n",
    "signature = infer_signature(input_data, output_example, params)\n",
    "signature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9b7f0b-8812-4cc0-bfb5-c8ede078daa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/26 21:41:57 INFO mlflow.models.utils: We convert input dictionaries to pandas DataFrames such that each key represents a column, collectively constituting a single row of data. If you would like to save data as multiple rows, please convert your data to a pandas DataFrame before passing to input_example.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2892370fb243bbb9f648c2c945b10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e65a784069410487a0a5bae24324e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2198e4c20e704c0bbf993dac7d60273f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/26 21:42:16 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - transformers (current: 4.51.3, required: transformers==4.46.3)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200c5b8e435a493da69b58c279e08759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82b180cc8ef4bc5baba9fd935509881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/repl_tmp_data/ReplId-19673-7356b-5/tmpsg7w8v7_/model/artifacts/gemma-2-2b-it/model-0000…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\nRegistered model 'cindy_demo_catalog.llm_fine_tuning.gemma_2_multi_adapters' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe72ae59403426181e0a79904011ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.us-west-2.amazonaws.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce49376062c429992b596728f8ba2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/repl_tmp_data/ReplId-19673-7356b-5/tmpsg7w8v7_/model/artifacts/gemma-2-2b-it/model-0000…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.amazonaws.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: databricks-e2demofieldengwest.s3.amazonaws.com. Connection pool size: 10\nCreated version '17' of model 'cindy_demo_catalog.llm_fine_tuning.gemma_2_multi_adapters'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6ba8cff46c46eab0239cf7a5533fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /tmp/tmpnsd5p6ti/model/artifacts/gemma-2-2b-it/model-00001-of-00002.safetensors:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5166675a346b4a2da2b21cbb39ae11de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: e2-demo-field-eng-dbfs.s3.us-west-2.amazonaws.com. Connection pool size: 10\n2025/04/26 21:43:25 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - transformers (current: 4.51.3, required: transformers==4.46.3)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d1cd7eb6e24119a21d6cb80be8a0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded adapter mappings\nloaded Peft Model /tmp/tmpnsd5p6ti/model/artifacts/adapters/gemma-2-2b-it-lora-sql\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-cd4e19dd-a617-4e48-94d1-f2f0e478ebef/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Peft Model /tmp/tmpnsd5p6ti/model/artifacts/adapters/gemma-2-2b-it-lora-jap-en\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Peft Model /tmp/tmpnsd5p6ti/model/artifacts/adapters/gemma-2-2b-it-lora-coder\ninput: ['what is Databricks?', \"what's ML\"]\nparams:  0.1 100 sql\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/26 21:43:37 INFO mlflow.tracking._tracking_service.client: 🏃 View run indecisive-gnu-724 at: e2-demo-field-eng.cloud.databricks.com/ml/experiments/2818997009296645/runs/5c3cb14999df45f287469ca03f89ddb1.\n2025/04/26 21:43:37 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: e2-demo-field-eng.cloud.databricks.com/ml/experiments/2818997009296645.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gemma_2_multi_adapters\"\n",
    "registered_model_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "\n",
    "artifacts = {\n",
    "    # \"tokenizer\": base_tokenizer_path,      \n",
    "    \"base_model\": base_model_path,  \n",
    "    \"adapters_mapping\": adapters_mapping_path,\n",
    "    \"adapters\": adapters_path\n",
    "            }\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:  \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        \"model\",\n",
    "        python_model=FINETUNED_QLORA(),\n",
    "        artifacts= artifacts,\n",
    "        pip_requirements=[\"torch==2.5.0\", \"torchvision==0.20.0\",\"transformers==4.46.3\", \"accelerate==1.1.1\", \"peft==0.15.2\", \"bitsandbytes==0.45.0\"],\n",
    "        input_example= input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=registered_model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9f4a0e-9b94-4734-acb5-681e9a6c9473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Load MLFLow Model locally to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35320ab1-7fc4-40a9-ada3-e4b6c2a59594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Restart cluster to avoid OOM if using a small GPU cluster (16G should be plenty for Gemma2 without restarting)\n",
    "#  dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315f4994-9fbb-4a1d-87ef-1909cf030b01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load registered model"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec539ad851b4fd7a303f74c7670a7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /local_disk0/repl_tmp_data/ReplId-19673-7356b-5/tmp7fx90dgq/model/artifacts/gemma-2-2b-it/model-00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6b7dcc65084a2295155cbf2107e92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/26 19:19:16 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - transformers (current: 4.51.3, required: transformers==4.46.3)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f39a5992da4beba478ae9e1da581a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded adapter mappings\nloaded Peft Model /local_disk0/repl_tmp_data/ReplId-19673-7356b-5/tmp7fx90dgq/model/artifacts/adapters/gemma-2-2b-it-lora-sql\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-cd4e19dd-a617-4e48-94d1-f2f0e478ebef/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Peft Model /local_disk0/repl_tmp_data/ReplId-19673-7356b-5/tmp7fx90dgq/model/artifacts/adapters/gemma-2-2b-it-lora-jap-en\nloaded Peft Model /local_disk0/repl_tmp_data/ReplId-19673-7356b-5/tmp7fx90dgq/model/artifacts/adapters/gemma-2-2b-it-lora-coder\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64d4dde-8058-48da-ae6e-692d0d57aad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['what is Databricks?', 'import pandas as']\nparams:  0.1 100 coder\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "input_data = {\n",
    "    \"prompts\":[\"what is Databricks?\",\"import pandas as\"]\n",
    "}\n",
    "params = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"coder\"\n",
    "    }\n",
    "\n",
    "preds = loaded_model.predict(input_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b1fbad-a570-424d-a7cd-88f6a401a3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['what is Databricks?\\n\\nYour task is to create a Python function that simulates the behavior of the `databricks_init` function. The function should take a list of arguments and return a dictionary containing the parsed arguments.\\n\\nThe `databricks_init` function is used to initialize a Databricks environment and is invoked with the following parameters:\\n- `args`: A list of arguments to be parsed.\\n\\nThe function should parse the arguments and return a dictionary containing the parsed arguments.\\n\\nYour task',\n",
       " \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Load the dataset\\ndata = pd.read_csv('data.csv')\\n\\n# Extract the 'age' and 'gender' columns\\nage = data['age']\\ngender = data['gender']\\n\\n# Create a bar chart to visualize the distribution of ages by gender\\nplt.bar(gender, age, color='skyblue')\\nplt.xlabel('Gender')\\nplt.ylabel('Age')\\n\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d6fbf27-3148-47bd-b37b-914b12a77548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Serve Registered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400f599c-9fd5-4543-9f4d-8b211d828095",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Endpoint Configs"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the name of the MLflow endpoint\n",
    "endpoint_name = \"gemma_2_multi_adapters\"\n",
    "\n",
    "# Get the latest version of the MLflow model\n",
    "model_version = model_info.registered_model_version\n",
    "print(model_version)\n",
    "# Name of the registered MLflow model\n",
    "registered_model_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# Get the API endpoint and token for the current notebook context\n",
    "API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "036e83ab-309e-4b59-a56b-9e6bc4b11543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create model endpoint for model serving. Gemma-2-2b-it can fit on a T4 GPU (GPU Small) with no quantization, choose GPU size based on loaded pyfunc model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03cd9f1d-792c-4618-ac04-ea69e96cdd72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create new endpoint"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n    \"error_code\": \"RESOURCE_ALREADY_EXISTS\",\n    \"message\": \"Endpoint with name 'gemma_2_multi_adapters' already exists.\",\n    \"details\": [\n        {\n            \"@type\": \"type.googleapis.com/google.rpc.RequestInfo\",\n            \"request_id\": \"0e1b4434-b56a-40d8-b485-bba450cc0b11\",\n            \"serving_data\": \"\"\n        }\n    ]\n}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.) \n",
    "workload_type = \"GPU_SMALL\" \n",
    "\n",
    "# Specify the scale-out size of compute (Small, Medium, Large, etc.)\n",
    "workload_size = \"Small\" \n",
    "\n",
    "# Specify Scale to Zero(only supported for CPU endpoints)\n",
    "scale_to_zero = True \n",
    "\n",
    "data = {\n",
    "    \"name\": endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"entity_name\": registered_model_name,\n",
    "                \"entity_version\": model_version,\n",
    "                \"workload_size\": workload_size,\n",
    "                \"scale_to_zero_enabled\": scale_to_zero,\n",
    "                \"workload_type\": workload_type,\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "response = requests.post(\n",
    "    url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n",
    ")\n",
    "\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dc70fc-dfc4-4a41-8db7-05aa90b6a1e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If endpoint already exists, you can update endpoint with the deisered model version or endpoint configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2452e5-6541-47d3-8f1a-6830570b7951",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update existing model endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "endpoint = client.update_endpoint(\n",
    "    endpoint=endpoint_name,\n",
    "    config={\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "              \"entity_name\": registered_model_name,\n",
    "                \"entity_version\": model_version,\n",
    "                \"scale_to_zero_enabled\": scale_to_zero,\n",
    "                \"workload_type\": workload_type,\n",
    "                \"workload_size\": workload_size\n",
    "\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "005f2b7d-751c-42e5-913e-f568ffc741dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View your endpoint\n",
    "To see more information about your endpoint, go to the Serving UI and search for your endpoint name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8281dac7-0cc1-4e8c-af47-81dfe4b67f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query your endpoint\n",
    "Once your endpoint is ready, you can query it by making an API request. Depending on the model size and complexity, it can take 30 minutes or more for the endpoint to get ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c56902e-72db-4324-a793-dec5611e427c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [\"what is Databricks?\\n\\n```sql\\nSELECT * FROM Databricks;```\\nThis query selects all columns and rows from the 'Databricks' table.\\n\", \"what's ML model performance for each model?\\nmodel\\n```sql\\nSELECT model_name, performance_score FROM model_performance;```\\nThis query retrieves the ML model performance for each model by selecting the model_name and performance_score columns from the model_performance table.\\n\"]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = {\n",
    "  \"dataframe_split\": {\n",
    "    \"columns\": [\n",
    "      \"prompts\"\n",
    "    ],\n",
    "    \"data\": [\n",
    "      [\n",
    "        [\n",
    "          \"what is Databricks?\",\n",
    "          \"what's ML\"\n",
    "        ]\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"params\": {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"sql\"\n",
    "  }\n",
    "}\n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "response = requests.post(\n",
    "    url=f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\", json=data, headers=headers\n",
    ")\n",
    "\n",
    "print(json.dumps(response.json()))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Serve Finetuned Gemma2 Model with Multiple Adapters",
   "widgets": {
    "huggingface_token": {
     "currentValue": "",
     "nuid": "a7268f62-77b4-433d-a9a2-1b9296963256",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "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",
      "label": "Enter Parameter",
      "name": "huggingface_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "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",
      "label": "Enter Parameter",
      "name": "huggingface_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}