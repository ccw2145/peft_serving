{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767fd7de-26e2-4668-b216-27da1954bfee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Serving Fine Tuned Gemma Model with multiple LoRA adapters on Databricks \n",
    "\n",
    "This is a tutorial to show how to serve [`gemma-2-2b-it`](https://huggingface.co/google/gemma-2-2b-it) with multiple LoRA adpaters on Databricks Model Serving.\n",
    "\n",
    "Environment for this notebook:\n",
    "- Runtime: 16.1 GPU ML Runtime\n",
    "- Instance: Tested on `g5.8xlarge` for AWS, smaller GPU cluster should also work\n",
    "- MLFlow 2.15\n",
    "\n",
    "Serving Endpoint requirement:\n",
    "- For this example, 1 T4 GPU is sufficient without further quantization (GPU Small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c5e29d-8cc6-402e-9593-0b893acc00fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install required packages\n",
    "\n",
    "Run the cells below to setup and install the required libraries. Since gemma-2-2b-it is small enought to fit in the cluster, we are not loading a quantized base model. However, for larger models (e.g. Llama 7 or 8B models), we can use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also need `accelerate`, `peft`, `transformers` to lload the base model and PEFT adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca4ea5a-33ff-4475-8b3b-bc4262dc2b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install bitsandbytes==0.45\n",
    "%pip install accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U transformers\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6100bd6-a927-4f55-bd55-07617595cbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] ='max_split_size_mb:128'\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed01c028-7468-4c18-9477-75a609431ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6171291-bce0-488a-9bd2-ccde21e0d619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading the model\n",
    "\n",
    "In this section we will load the [gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) model and a few popular open source adapters from Huggingface and save to Unity Catalog Volumes.\n",
    "\n",
    "Adapters we will be using here:\n",
    "  - [google-cloud-partnership/gemma-2-2b-it-lora-sql](https://huggingface.co/google-cloud-partnership/gemma-2-2b-it-lora-sql)\n",
    "  - [google-cloud-partnership/gemma-2-2b-it-lora-jap-en](https://huggingface.co/google-cloud-partnership/gemma-2-2b-it-lora-jap-en)\n",
    "  - [google-cloud-partnership/gemma-2-2b-it-lora-magicoder](https://huggingface.co/google-cloud-partnership/gemma-2-2b-it-lora-magicoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b04249-cd47-4d94-a92f-a63596f2ddcd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Catalog to use"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"cindy_demo_catalog\"\n",
    "schema = \"llm_fine_tuning\"\n",
    "volume = \"hf_models\"\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14cd1ed-83e6-4b6f-af0e-a19fc8e8b9a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Specify model and adapters paths"
    }
   },
   "outputs": [],
   "source": [
    "base_model_path = f'/Volumes/{catalog}/{schema}/{volume}/gemma-2-2b-it'\n",
    "adapters_path = f'/Volumes/{catalog}/{schema}/{volume}/adapters' # Directory to store all adapters\n",
    "adapters_mapping_path = f'/Volumes/{catalog}/{schema}/{volume}/adapters_mapping.json' # Mapping of adapters to model names\n",
    "adapters_mapping = {'sql' :'gemma-2-2b-it-lora-sql',\n",
    "                    'japanese': 'gemma-2-2b-it-lora-jap-en',\n",
    "                    'coder': 'gemma-2-2b-it-lora-coder'\n",
    "                    }\n",
    "\n",
    "# base_tokenizer_path = f'/Volumes/{catalog}/{schema}/{volume}/gemma-2-2b-it-tokenzier' ## Specify this if tokenizer is not stored with the base model and has a different path\n",
    "\n",
    "import os\n",
    "if not os.path.exists(adapters_path):\n",
    "    dbutils.fs.mkdirs(adapters_path)\n",
    "    \n",
    "import json\n",
    "with open(adapters_mapping_path, 'w') as f:\n",
    "    json.dump(adapters_mapping, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b1e3fd-d9bc-4913-a817-373f01534901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Download base model and adapters to Unity Catalog Volumes\n",
    "- Requires Huggingface Token with access to use gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f373e17-9c12-4879-8c10-80275c9da766",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add HF token if needed"
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\"huggingface_token\", \"\", \"Enter Parameter\")\n",
    "# os.environ['HF_TOKEN'] = dbutils.widgets.get(\"huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05696cce-33f7-4714-8634-9172c3d79075",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download models to UC Volumes"
    }
   },
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# # Download base Gemma model\n",
    "# base_model_path = snapshot_download(repo_id=\"google/gemma-2-2b-it\", local_dir=base_model_path)\n",
    "\n",
    "# # Download LoRA adapters\n",
    "# lora_sql_path = snapshot_download(repo_id=\"google-cloud-partnership/gemma-2-2b-it-lora-sql\", local_dir=f\"{adapters_path}/gemma-2-2b-it-lora-sql\")\n",
    "# lora_jap_en_path = snapshot_download(repo_id=\"google-cloud-partnership/gemma-2-2b-it-lora-jap-en\", local_dir=f\"{adapters_path}/gemma-2-2b-it-lora-jap-en\")\n",
    "# lora_coder_path = snapshot_download(repo_id='google-cloud-partnership/gemma-2-2b-it-lora-magicoder', local_dir=f\"{adapters_path}/gemma-2-2b-it-lora-coder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "040401ab-b246-4b1a-a430-6072ea50be0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Load all adapters and test peft model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32ee337-e942-41b7-bddf-ec18540ec62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "# from peft import PeftModel\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map=\"cuda:0\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "# peft_model = base_model \n",
    "\n",
    "# for name, path in adapters_mapping.items():\n",
    "#   # This loads the adapter onto the model under the provided adapter name.\n",
    "#   peft_model.load_adapter(f\"{adapters_path}/{path}\", adapter_name=name)\n",
    "#   print('loaded Peft Model',name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17921749-587e-46eb-b610-425227ac9958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84366ff8-bcea-4aed-8a50-9460f5000ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## peft_model.delete_adapter(\"sql\") ## To remove an adapter from peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd0a82b-9921-4383-bd01-fc62d1855dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create MLFlow PyFunc Model with Multiple Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea0b356-a293-46d2-b25f-0609a655873e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "class FINETUNED_QLORA(mlflow.pyfunc.PythonModel):\n",
    "    # Load base model, tokenizer, and adapters.\n",
    "    def load_context(self, context):\n",
    "        import json\n",
    "        import os\n",
    "\n",
    "        ## Uncomment this to load a quanitized model, requires less memory, slower inference due to de-quant overhead\n",
    "        # bnb_config = BitsAndBytesConfig(\n",
    "        # load_in_4bit=True,\n",
    "        # bnb_4bit_quant_type=\"nf4\",\n",
    "        # # bnb_4bit_use_double_quant=True,\n",
    "        # bnb_4bit_compute_dtype=torch.float16,\n",
    "        # )\n",
    "\n",
    "        # Load the tokenizer and set the pad token to the EOS token.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(context.artifacts['base_model'])\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load the base model (using 4-bit quantization in this example).\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            context.artifacts['base_model'], \n",
    "            return_dict=True, \n",
    "            # quantization_config=bnb_config, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\": 0})\n",
    "        \n",
    "        # Load PEFT adapters from a dictionary artifact.\n",
    "        with open(context.artifacts[\"adapters_mapping\"], \"r\") as f:\n",
    "            self.adapters_mapping = json.load(f)\n",
    "\n",
    "        print('loaded adapter mappings')\n",
    "       \n",
    "        self.model = self.base_model\n",
    "  \n",
    "        for adapter_name, adapter_path in self.adapters_mapping.items():\n",
    "            self.model.load_adapter(f\"{context.artifacts[\"adapters\"]}/{adapter_path}\", adapter_name=adapter_name)\n",
    "            print('loaded Peft Model',f\"{context.artifacts[\"adapters\"]}/{adapter_path}\")\n",
    "\n",
    "        ## Set the model to evaluation mode. Use this for a merged model\n",
    "        # self.model.eval()\n",
    "\n",
    "        self.model.config.use_cache = False\n",
    "\n",
    "    def predict(self, context, model_input, params):\n",
    "        # Handle single or batch prompts, input should be a list[str]\n",
    "        prompts = model_input.get(\"prompts\")[0]\n",
    "\n",
    "        print('input:', prompts)\n",
    "        \n",
    "        temperature = float(params.get('temperature', 0.1))\n",
    "        max_tokens = int(params.get('max_tokens', 100))\n",
    "        adapter_name = params.get('adapter', 'sql')\n",
    "        print( 'params: ', temperature, max_tokens, adapter_name)\n",
    "\n",
    "        # Activate the desired adapter if provided.\n",
    "        if adapter_name in list(self.adapters_mapping.keys()):\n",
    "          self.model.set_adapter(adapter_name)\n",
    "\n",
    "        else:\n",
    "          print('no adapter found')\n",
    "          generated_text = 'no adapter found'\n",
    "          return generated_text\n",
    "        \n",
    "        # Tokenize the input prompt with padding and truncation, and move to CUDA.\n",
    "        batch = self.tokenizer(text=prompts, padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output_tokens = self.model.generate(\n",
    "                input_ids=batch.input_ids, \n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text.\n",
    "        generated_texts = self.tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "        return generated_texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "639cc6af-30f4-452f-bc75-df9304dbb1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Test MLFlow PyFunc Model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70e5da5-88d1-47d0-b934-3e7243998f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a local model context object with required artifact paths for testing\n",
    "artifacts = {\n",
    "    \"base_model\": base_model_path,  \n",
    "    \"adapters_mapping\": adapters_mapping_path,\n",
    "    \"adapters\": adapters_path\n",
    "            }\n",
    "\n",
    "class ModelContext:\n",
    "    def __init__(self):\n",
    "        self.artifacts = artifacts\n",
    "# Instantiate a dummy context.\n",
    "dummy_context = ModelContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec29c321-0d41-4bc0-a5d7-c1318909ba85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate your pyfunc wrapper and load the context.\n",
    "finetuned_model = FINETUNED_QLORA()\n",
    "finetuned_model.load_context(dummy_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d64f5e-e1b3-42f1-a8e0-e49e4df94a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_input =  {'prompts': [\"what is Databricks?\", \"what's ML\"]}\n",
    "params = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"sql\"\n",
    "}\n",
    "# Run a prediction and display the output.\n",
    "print(\"Testing prediction...\")\n",
    "generated_output = finetuned_model.predict(dummy_context, test_input, params)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be35c21-558d-4454-aa01-af069bd7db52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488b8c0b-845c-47fa-a7e0-35a6c0231c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_input = {\n",
    "    \"prompts\": [\"データブリックスとは\"],\n",
    "}\n",
    "\n",
    "# Run a prediction and display the output.\n",
    "print(\"Testing prediction...\")\n",
    "generated_output = finetuned_model.predict(dummy_context, test_input, params)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0a7f5d-7e26-45b4-8853-6daef6b490a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_input = {\n",
    "    \"prompts\": [\"generate some pandas code to create a dataframe\"]\n",
    "}\n",
    "\n",
    "# Run a prediction and display the output.\n",
    "print(\"Testing prediction...\")\n",
    "generated_output = finetuned_model.predict(dummy_context, test_input, params)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "906ee0e4-2bd4-4fb2-b131-ce43058a8ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log to MLFlow + Register model in UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa3cfce-5170-4f03-9461-9fa93bbd3954",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Input Example and Model Signature"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "from mlflow.types.schema import Array, DataType, Schema\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set mlflow registry to databricks-uc\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# Specify an input example that conforms to the input schema for the task.\n",
    "import numpy as np\n",
    "input_data={\"prompts\": [\"what is Databricks?\", \"what's ML\"]}\n",
    "               \n",
    "params = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"sql\"\n",
    "}\n",
    "input_example = (input_data, params)\n",
    "\n",
    "output_example = {\"generated_texts\": [\"what is Databricks?\\n\\n```sql\\nSELECT * FROM Databricks;```\\nThis query retrieves all records from the 'Databricks' table.\\n\", \"what's ML model performance for each model?\\nmodel\\n```sql\\nSELECT model_name, performance_score FROM model_performance;```\\nThis query retrieves the ML model performance for each model by selecting the model_name and performance_score columns from the model_performance table.\\n\"]}\n",
    "\n",
    "signature = infer_signature(input_data, output_example, params)\n",
    "signature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9b7f0b-8812-4cc0-bfb5-c8ede078daa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"gemma_2_multi_adapters\"\n",
    "registered_model_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "\n",
    "artifacts = {\n",
    "    # \"tokenizer\": base_tokenizer_path,      \n",
    "    \"base_model\": base_model_path,  \n",
    "    \"adapters_mapping\": adapters_mapping_path,\n",
    "    \"adapters\": adapters_path\n",
    "            }\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:  \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        \"model\",\n",
    "        python_model=FINETUNED_QLORA(),\n",
    "        artifacts= artifacts,\n",
    "        pip_requirements=[\"torch==2.5.0\", \"torchvision==0.20.0\",\"transformers==4.46.3\", \"accelerate==1.1.1\", \"peft==0.15.2\", \"bitsandbytes==0.45.0\"],\n",
    "        input_example= input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=registered_model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9f4a0e-9b94-4734-acb5-681e9a6c9473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Load MLFLow Model locally to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35320ab1-7fc4-40a9-ada3-e4b6c2a59594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Restart cluster to avoid OOM if using a small GPU cluster (16G should be plenty for Gemma2 without restarting)\n",
    "#  dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315f4994-9fbb-4a1d-87ef-1909cf030b01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load registered model"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64d4dde-8058-48da-ae6e-692d0d57aad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "input_data = {\n",
    "    \"prompts\":[\"what is Databricks?\",\"import pandas as\"]\n",
    "}\n",
    "params = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"coder\"\n",
    "    }\n",
    "\n",
    "preds = loaded_model.predict(input_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b1fbad-a570-424d-a7cd-88f6a401a3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d6fbf27-3148-47bd-b37b-914b12a77548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Serve Registered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400f599c-9fd5-4543-9f4d-8b211d828095",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Endpoint Configs"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the name of the MLflow endpoint\n",
    "endpoint_name = \"gemma_2_multi_adapters\"\n",
    "\n",
    "# Get the latest version of the MLflow model\n",
    "model_version = model_info.registered_model_version\n",
    "print(model_version)\n",
    "# Name of the registered MLflow model\n",
    "registered_model_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# Get the API endpoint and token for the current notebook context\n",
    "API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "036e83ab-309e-4b59-a56b-9e6bc4b11543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create model endpoint for model serving. Gemma-2-2b-it can fit on a T4 GPU (GPU Small) with no quantization, choose GPU size based on loaded pyfunc model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03cd9f1d-792c-4618-ac04-ea69e96cdd72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create new endpoint"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.) \n",
    "workload_type = \"GPU_SMALL\" \n",
    "\n",
    "# Specify the scale-out size of compute (Small, Medium, Large, etc.)\n",
    "workload_size = \"Small\" \n",
    "\n",
    "# Specify Scale to Zero(only supported for CPU endpoints)\n",
    "scale_to_zero = True \n",
    "\n",
    "data = {\n",
    "    \"name\": endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"entity_name\": registered_model_name,\n",
    "                \"entity_version\": model_version,\n",
    "                \"workload_size\": workload_size,\n",
    "                \"scale_to_zero_enabled\": scale_to_zero,\n",
    "                \"workload_type\": workload_type,\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "response = requests.post(\n",
    "    url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n",
    ")\n",
    "\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dc70fc-dfc4-4a41-8db7-05aa90b6a1e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If endpoint already exists, you can update endpoint with the deisered model version or endpoint configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2452e5-6541-47d3-8f1a-6830570b7951",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update existing model endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "endpoint = client.update_endpoint(\n",
    "    endpoint=endpoint_name,\n",
    "    config={\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "              \"entity_name\": registered_model_name,\n",
    "                \"entity_version\": model_version,\n",
    "                \"scale_to_zero_enabled\": scale_to_zero,\n",
    "                \"workload_type\": workload_type,\n",
    "                \"workload_size\": workload_size\n",
    "\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "005f2b7d-751c-42e5-913e-f568ffc741dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View your endpoint\n",
    "To see more information about your endpoint, go to the Serving UI and search for your endpoint name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8281dac7-0cc1-4e8c-af47-81dfe4b67f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query your endpoint\n",
    "Once your endpoint is ready, you can query it by making an API request. Depending on the model size and complexity, it can take 30 minutes or more for the endpoint to get ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c56902e-72db-4324-a793-dec5611e427c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "  \"dataframe_split\": {\n",
    "    \"columns\": [\n",
    "      \"prompts\"\n",
    "    ],\n",
    "    \"data\": [\n",
    "      [\n",
    "        [\n",
    "          \"what is Databricks?\",\n",
    "          \"what's ML\"\n",
    "        ]\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"params\": {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"adapter\": \"sql\"\n",
    "  }\n",
    "}\n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "response = requests.post(\n",
    "    url=f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\", json=data, headers=headers\n",
    ")\n",
    "\n",
    "print(json.dumps(response.json()))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Serve Finetuned Gemma2 Model with Multiple Adapters",
   "widgets": {
    "huggingface_token": {
     "currentValue": "",
     "nuid": "a7268f62-77b4-433d-a9a2-1b9296963256",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "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",
      "label": "Enter Parameter",
      "name": "huggingface_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "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",
      "label": "Enter Parameter",
      "name": "huggingface_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
